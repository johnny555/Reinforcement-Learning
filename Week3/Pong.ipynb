{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"~/git/swiftai\")' SwiftAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import SwiftAI\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public typealias TF = Tensor<Float>\n",
    "public typealias TI = Tensor<Int32>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Python\n",
    "\n",
    "let gym = Python.import(\"gym\")\n",
    "let np = Python.import(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct Observation {\n",
    "    public let obs: TF\n",
    "}\n",
    "\n",
    "let empty_obs = Observation(obs: TF(zeros: [1, 210, 160, 3]))\n",
    "\n",
    "struct Action {\n",
    "    public let action: Int\n",
    "}\n",
    "\n",
    "struct Reward : AdditiveArithmetic {\n",
    "    public let r: Float\n",
    "    \n",
    "}\n",
    "let zero_reward = Reward(r:0)\n",
    "\n",
    "// Define how to add reward objects\n",
    "func +(lhs: Reward, rhs: Reward) -> Reward {\n",
    "        let r = lhs.r + rhs.r\n",
    "        return Reward(r:r)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to deal with the Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func np_to_tf(_ arr: PythonObject) -> TF {\n",
    "    return Tensor<Float> ( numpy: np.array(arr.reshape([1, 210,160,3]), dtype: np.float32))!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func np_to_obs(_ arr: PythonObject) -> Observation {\n",
    "    let obs = Tensor<Float> ( numpy: np.array(arr.reshape([1, 210,160,3]), dtype: np.float32))\n",
    "    if obs == nil {\n",
    "        print(\"np_to_obs: Oh no! observation did not parse!\")\n",
    "    }\n",
    "    return Observation(obs: obs!)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func reset_env(_ env: PythonObject) -> Observation {\n",
    "    return np_to_obs(env.reset())\n",
    "}\n",
    "\n",
    "func step_env(env: PythonObject, act: Action) -> (Observation, Reward, Bool) {\n",
    "    let res = env.step(act.action)\n",
    "    let obs = np_to_obs(res[0])\n",
    "    let r = Float(res[1])!\n",
    "    let cont = Bool(res[2])!\n",
    "    \n",
    "    return (obs, Reward(r:r), cont )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// steps n times, summing reward over steps.\n",
    "func step_n_times(n: Int = 4, env: PythonObject, act: Action) ->\n",
    "    (Observation, Reward, Bool)\n",
    "{\n",
    "    var rew = Reward(r: 0)\n",
    "\n",
    "    var res = env.step(act.action)\n",
    "    rew = rew + Reward(r: Float(res[1])! )\n",
    "    \n",
    "    let range: ClosedRange<Int> = 1...n\n",
    "    for _ in range {\n",
    "        res = env.step(act.action)\n",
    "        rew = rew + Reward(r: Float(res[1])! )\n",
    "    }\n",
    "    \n",
    "    let obs = np_to_obs(res[0])\n",
    "    let running  = Bool(res[2])!\n",
    "    \n",
    "    return (obs, rew, running)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try to build a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct DQNModel: Layer {\n",
    "    public var conv1: ConvLayer\n",
    "    public var conv2: ConvLayer\n",
    "    public var conv3: ConvLayer\n",
    "    public var pool = GlobalAvgPool2D<Float>()\n",
    "    public var linear1: Dense<Float>\n",
    "    public var linear2: Dense<Float>\n",
    "\n",
    "    \n",
    "    public init(nActions: Int){\n",
    "        conv1 = ConvLayer(3, 32, ks: 8)\n",
    "        conv2 = ConvLayer(32, 64, ks: 4, stride: 2)\n",
    "        conv3 = ConvLayer(64, 64, ks: 3)\n",
    "        linear1 = Dense(inputSize: 64, outputSize: 256, activation: relu) \n",
    "        linear2 = Dense(inputSize: 256, outputSize: nActions) \n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func callAsFunction(_ input: TF) -> TF {\n",
    "       return input.compose(conv1, conv2, conv3, pool, linear1, linear2)\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct AgentHyperParams {\n",
    "    public var epsilon_start : Float = 1\n",
    "    public var epsilon_decay : Float = 0.000001\n",
    "    public var epsilon_final : Float = 0.02\n",
    "    public let learning_rate : Float\n",
    "    public let gamma: Float\n",
    "    public let num_actions: Int\n",
    "    \n",
    "    public init(lr: Float, ga: Float, na: Int) {\n",
    "        learning_rate=lr\n",
    "        gamma = ga\n",
    "        num_actions = na\n",
    "    }\n",
    "}\n",
    "var params =  AgentHyperParams(lr:0.8, ga:0.99, na:6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let random = Python.import(\"numpy.random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let vvv = agent.transition_obs_buffer[0].obs\n",
    "vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct PongBatch<Obs: Differentiable & TensorGroup,\n",
    "                        Act: Differentiable & TensorGroup,\n",
    "                        Labels: TensorGroup>: TensorGroup {\n",
    "    public var x_obs: Obs\n",
    "    public var x_act: Act\n",
    "   \n",
    "    public var yb: Labels\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let bb = PongBatch(x_obs:vvv, x_act:vvv, yb:vvv)\n",
    "let tds = Dataset(elements: bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds.batched(32).shuffled(sampleCount:1, randomSeed:42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = Tensor(randomStandardUniform: [10,10])\n",
    "print(type(of:t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.concatenated(with: t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Agent {\n",
    "    public var model: DQNModel\n",
    "    public var target: DQNModel    // The target is frozen while the model learns against it.\n",
    "    \n",
    "    public var transition_obs_buffer: TF \n",
    "    public var transition_act_buffer: TI \n",
    "    public var transition_rew_buffer: [Int] \n",
    "    public var transition_done_buffer: [Bool] \n",
    "    \n",
    "    public var epsilon : Float\n",
    "    public var params: AgentHyperParams\n",
    "    \n",
    "    public var steps: Int // Number of training steps taken\n",
    "    \n",
    "    public init(par: AgentHyperParams) {\n",
    "        params = par\n",
    "\n",
    "        model = DQNModel(nActions: params.num_actions)\n",
    "        target = DQNModel(nActions: params.num_actions)\n",
    "        target.variables = model.variables // hopefully this will do a copy not pass by reference... \n",
    "    \n",
    "        epsilon = params.epsilon_start\n",
    "        steps = 0\n",
    "\n",
    "    }\n",
    "    \n",
    "    public mutating func act_eps_greedy(obs: Observation) -> Action {\n",
    "        let r = Float( random.random() )!\n",
    "        steps += 1\n",
    "\n",
    "        \n",
    "        if r < epsilon {\n",
    "              let a = Int (random.randint(6) )!\n",
    "              return Action(action: a)\n",
    "        }\n",
    "        print(\"Taking real action\")\n",
    "        //let a = Int (random.randint(6) )!\n",
    "         \n",
    "        let a = Int(target(obs.obs).argmax().scalar!)\n",
    "        \n",
    "        return Action(action: a )\n",
    "    }\n",
    "    \n",
    "    \n",
    "    public mutating func sample_and_optimize(batch_size: Int) -> () {\n",
    "        // probably a bad heuristic... \n",
    "        let n_samples = transition_obs_buffer.shape[0]\n",
    "        \n",
    "        if n_samples < batch_size*16 {\n",
    "            return\n",
    "        }\n",
    "        \n",
    "        let index = batchedRanges(start: 0, end: n_samples, bs: batch_size)\n",
    "        //meanAbsoluteError(observed, expected)\n",
    "        // now sample randomly and learn. \n",
    "    }\n",
    "    \n",
    "    \n",
    "    public mutating func add_env_feedback(obs: Observation, act: Action, rew: Reward, done: Bool) -> () {\n",
    "        transition_obs_buffer.concatenated(with: obs.obs)\n",
    "        transition_act_buffer.concatenated(with: act.action)\n",
    "        transition_rew_buffer.append(rew)\n",
    "        transition_done_buffer.append(done)\n",
    "        epsilon = max(params.epsilon_final, params.epsilon_start - Float(steps)*params.epsilon_decay)\n",
    "\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var p1 = DQNModel(nActions: 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var agent = Agent(par: params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var  env = gym.make(\"PongNoFrameskip-v4\")\n",
    "var s1 = env.reset().reshape([1, 210,160,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// how many actions are there... \n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var current_obs : Observation\n",
    "var next_obs : Observation\n",
    "var rew : Reward\n",
    "var finished: Bool = false\n",
    "next_obs = Observation(obs: np_to_tf(s1))\n",
    "\n",
    "let total_games = 10\n",
    "var count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (count < total_games) {\n",
    "    current_obs = next_obs\n",
    "    \n",
    "    let act: Action = agent.act_eps_greedy(obs: current_obs)\n",
    "    \n",
    "    (next_obs , rew, finished) = step_n_times(env: env, act:act )\n",
    "    \n",
    "    agent.add_env_feedback(obs: current_obs, act: act, rew: rew, done: finished )\n",
    "    agent.sample_and_optimize(batch_size: 32)\n",
    "    \n",
    "    if finished {\n",
    "        count += 1\n",
    "        next_obs = reset_env(env)\n",
    "        print(\"game \" + String(count) + \" starting \")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
