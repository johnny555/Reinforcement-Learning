{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"~/git/swiftai\")' SwiftAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import SwiftAI\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public typealias TF = Tensor<Float>\n",
    "public typealias TI = Tensor<Int32>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Python\n",
    "\n",
    "let gym = Python.import(\"gym\")\n",
    "let np = Python.import(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct Observation {\n",
    "    public let obs: TF\n",
    "}\n",
    "\n",
    "let empty_obs = Observation(obs: TF(zeros: [1, 210, 160, 3]))\n",
    "\n",
    "struct Action {\n",
    "    public let action: Int\n",
    "}\n",
    "\n",
    "struct Reward : AdditiveArithmetic {\n",
    "    public let r: Float\n",
    "    \n",
    "}\n",
    "let zero_reward = Reward(r:0)\n",
    "\n",
    "// Define how to add reward objects\n",
    "func +(lhs: Reward, rhs: Reward) -> Reward {\n",
    "        let r = lhs.r + rhs.r\n",
    "        return Reward(r:r)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to deal with the Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func np_to_tf(_ arr: PythonObject) -> TF {\n",
    "    return Tensor<Float> ( numpy: np.array(arr.reshape([1, 210,160,3]), dtype: np.float32))!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func np_to_obs(_ arr: PythonObject) -> Observation {\n",
    "    let obs = Tensor<Float> ( numpy: np.array(arr.reshape([1, 210,160,3]), dtype: np.float32))\n",
    "    if obs == nil {\n",
    "        print(\"np_to_obs: Oh no! observation did not parse!\")\n",
    "    }\n",
    "    return Observation(obs: obs!)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func reset_env(_ env: PythonObject) -> Observation {\n",
    "    return np_to_obs(env.reset())\n",
    "}\n",
    "\n",
    "func step_env(env: PythonObject, act: Action) -> (Observation, Reward, Bool) {\n",
    "    let res = env.step(act.action)\n",
    "    let obs = np_to_obs(res[0])\n",
    "    let r = Float(res[1])!\n",
    "    let cont = Bool(res[2])!\n",
    "    \n",
    "    return (obs, Reward(r:r), cont )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// steps n times, summing reward over steps.\n",
    "func step_n_times(n: Int = 4, env: PythonObject, act: Action) ->\n",
    "    (Observation, Reward, Bool)\n",
    "{\n",
    "    var rew = Reward(r: 0)\n",
    "\n",
    "    var res = env.step(act.action)\n",
    "    rew = rew + Reward(r: Float(res[1])! )\n",
    "    \n",
    "    let range: ClosedRange<Int> = 1...n\n",
    "    for _ in range {\n",
    "        res = env.step(act.action)\n",
    "        rew = rew + Reward(r: Float(res[1])! )\n",
    "    }\n",
    "    \n",
    "    let obs = np_to_obs(res[0])\n",
    "    let running  = Bool(res[2])!\n",
    "    \n",
    "    return (obs, rew, running)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try to build a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct DQNModel: Layer {\n",
    "    public var conv1: ConvLayer\n",
    "    public var conv2: ConvLayer\n",
    "    public var conv3: ConvLayer\n",
    "    public var pool = GlobalAvgPool2D<Float>()\n",
    "    public var linear1: Dense<Float>\n",
    "    public var linear2: Dense<Float>\n",
    "\n",
    "    \n",
    "    public init(nActions: Int){\n",
    "        conv1 = ConvLayer(3, 32, ks: 8)\n",
    "        conv2 = ConvLayer(32, 64, ks: 4, stride: 2)\n",
    "        conv3 = ConvLayer(64, 64, ks: 3)\n",
    "        linear1 = Dense(inputSize: 64, outputSize: 256, activation: relu) \n",
    "        linear2 = Dense(inputSize: 256, outputSize: nActions) \n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func callAsFunction(_ input: TF) -> TF {\n",
    "       return input.compose(conv1, conv2, conv3, pool, linear1, linear2)\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct AgentHyperParams {\n",
    "    public var epsilon_start : Float = 1\n",
    "    public var epsilon_decay : Float = 0.000001\n",
    "    public var epsilon_final : Float = 0.02\n",
    "    public let learning_rate : Float\n",
    "    public let gamma: Float\n",
    "    public let num_actions: Int\n",
    "    \n",
    "    public init(lr: Float, ga: Float, na: Int) {\n",
    "        learning_rate=lr\n",
    "        gamma = ga\n",
    "        num_actions = na\n",
    "    }\n",
    "}\n",
    "var params =  AgentHyperParams(lr:0.8, ga:0.99, na:6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let random = Python.import(\"numpy.random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct Agent {\n",
    "    public var model: DQNModel\n",
    "    public var target: DQNModel    // The target is frozen while the model learns against it.\n",
    "    \n",
    "    public var transition_obs_buffer: [Observation]\n",
    "    public var transition_act_buffer: [Action]\n",
    "    public var transition_rew_buffer: [Reward]\n",
    "    \n",
    "    public var epsilon : Float\n",
    "    public var params: AgentHyperParams\n",
    "    \n",
    "    public var steps: Int // Number of training steps taken\n",
    "    \n",
    "    public init(par: AgentHyperParams) {\n",
    "        params = par\n",
    "\n",
    "        model = DQNModel(nActions: params.num_actions)\n",
    "        target = DQNModel(nActions: params.num_actions)\n",
    "        target.variables = model.variables // hopefully this will do a copy not pass by reference... \n",
    "        \n",
    "        transition_obs_buffer = []\n",
    "        transition_act_buffer = []   \n",
    "        transition_rew_buffer = []   \n",
    "        epsilon = params.epsilon_start\n",
    "        steps = 0\n",
    "\n",
    "    }\n",
    "    \n",
    "    \n",
    "    public mutating func act_eps_greedy(obs: Observation) -> Action {\n",
    "        let r = Float( random.random() )!\n",
    "        steps += 1\n",
    "\n",
    "        \n",
    "        if r < epsilon {\n",
    "              let a = Int (random.randint(6) )!\n",
    "              return Action(action: a)\n",
    "        }\n",
    "        print(\"Taking real action\")\n",
    "        //let a = Int (random.randint(6) )!\n",
    "         \n",
    "        let a = Int(target(obs.obs).argmax().scalar!)\n",
    "        \n",
    "        return Action(action: a )\n",
    "    }\n",
    "    \n",
    "    \n",
    "    public mutating func sample_and_optimize(batch_size: Int) -> () {\n",
    "        // probably a bad heuristic... \n",
    "        if transition_obs_buffer.count < batch_size*16 {\n",
    "            return\n",
    "        }\n",
    "        \n",
    "        // now sample randomly and learn. \n",
    "    }\n",
    "    \n",
    "    \n",
    "    public mutating func add_env_feedback(obs: Observation, act: Action, rew: Reward) -> () {\n",
    "        transition_obs_buffer.append(obs)\n",
    "        transition_act_buffer.append(act)\n",
    "        transition_rew_buffer.append(rew)\n",
    "        epsilon = max(params.epsilon_final, params.epsilon_start - Float(steps)*params.epsilon_decay)\n",
    "\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var p1 = DQNModel(nActions: 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var vars = p1.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var agent = Agent(par: params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.variables = vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var  env = gym.make(\"PongNoFrameskip-v4\")\n",
    "var s1 = env.reset().reshape([1, 210,160,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// how many actions are there... \n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var current_obs : Observation\n",
    "var next_obs : Observation\n",
    "var rew : Reward\n",
    "var finished: Bool = false\n",
    "next_obs = Observation(obs: np_to_tf(s1))\n",
    "\n",
    "let total_games = 10\n",
    "var count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (count < total_games) {\n",
    "    current_obs = next_obs\n",
    "    \n",
    "    let act: Action = agent.act_eps_greedy(obs: current_obs)\n",
    "    \n",
    "    (next_obs , rew, finished) = step_n_times(env: env, act:act )\n",
    "    \n",
    "    agent.add_env_feedback(obs: current_obs, act: act, rew: rew )\n",
    "    agent.sample_and_optimize(batch_size: 32)\n",
    "    \n",
    "    if finished {\n",
    "        count += 1\n",
    "        next_obs = reset_env(env)\n",
    "        print(\"game \" + String(count) + \" starting \")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var a = agent.transition_obs_buffer[0].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int(agent.target(a).argmax().scalar!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (a,b,c) = step_n_times(n:10, env: env, act: Action(action:1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.transition_obs_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.obs - np_to_tf(s1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var r = np_to_tf(s1)\n",
    "var res = p1(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var a = res.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int(a.scalar!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func select_greedy_action(s: SA, \n",
    "                          obs: Observation) -> Action {\n",
    "    \n",
    "    let (_, a) = best_action_value(s:s, cs:obs)\n",
    "    \n",
    "    return a\n",
    "}\n",
    "\n",
    "func best_action_value(s: SA, \n",
    "                       cs: Observation) -> (Reward, Action)\n",
    "{\n",
    "    var best_value: Float = 0\n",
    "    var best_action = Action(action:0)\n",
    "    let r : Range = 0..<4\n",
    "    for i in r {\n",
    "        var a = Action(action:i)\n",
    "        var pair  = StateActionPair(state: cs, act:a) \n",
    "        if ( (s[pair] ?? 0) > best_value) {\n",
    "            best_value = s[pair] ?? 0\n",
    "            best_action = a\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return (Reward(reward:best_value), \n",
    "            best_action)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func select_eps_greedy_action(s: SA, \n",
    "                              obs: Observation, \n",
    "                              n_actions: Int, \n",
    "                              eps: Float) -> Action {\n",
    "    \n",
    "    let (val, act) = best_action_value(s:s, cs: obs)\n",
    "    \n",
    "    let r = Float( random.random() )!\n",
    "    \n",
    "    if (r < eps) {\n",
    "        let r_act = Int(random.randint(0, n_actions))!\n",
    "        return Action(action: r_act)\n",
    "        \n",
    "    } else {\n",
    "        return act\n",
    "    }\n",
    "}\n",
    "\n",
    "func q_learning(sr:inout SA, current_obs: Observation, \n",
    "                next_obs: Observation, r: Reward, a: Action, \n",
    "                g: Float, lr: Float) -> SA\n",
    "{\n",
    "    let (best_value, _) = best_action_value(s:sr, \n",
    "                                             cs: next_obs)\n",
    "    \n",
    "    let Q_target = r.reward + g * best_value.reward\n",
    "    let pair = StateActionPair(state: current_obs, act:a)\n",
    "    let Q_error = Q_target - (sr[pair] ?? 0)\n",
    "    \n",
    "    \n",
    "    sr[pair] = (sr[pair] ?? 0) + lr * Q_error\n",
    "    \n",
    "    return sr\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let Gamma : Float = 0.95\n",
    "\n",
    "var epsilon: Float = 1.0\n",
    "let EPS_DECAY_RATE : Float = 0.99939\n",
    "let LEARNING_RATE : Float = 0.8\n",
    "\n",
    "let TEST_EPISODES = 100\n",
    "let MAX_GAMES = 150001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_count = 0\n",
    "epsilon = 1.0\n",
    "\n",
    "var total_reward : Float = 0\n",
    "\n",
    "while (games_count < MAX_GAMES) {\n",
    "    let act = select_eps_greedy_action(s: sr, obs:obs, \n",
    "                                       n_actions:n_actions,\n",
    "                                       eps: epsilon\n",
    "                                      )\n",
    "    \n",
    "    let (next_obs, reward, done) = step_env(env: env, act:act)\n",
    "    sr = q_learning(sr: &sr, current_obs: obs, next_obs:next_obs, \n",
    "                   r: reward, a: act, g: Gamma,\n",
    "                   lr: LEARNING_RATE)\n",
    "    obs = next_obs\n",
    "    total_reward += reward.reward\n",
    "    //print(total_reward)\n",
    "    if (done) {\n",
    "        epsilon *= EPS_DECAY_RATE\n",
    "    \n",
    "        if ((games_count % 1000) == 0) {\n",
    "            let test_reward = test_game(env: env, s: sr, no_games: TEST_EPISODES)\n",
    "            test_rewards_list.append(test_reward)\n",
    "            print(\"Games count: \" + String(games_count) + \" Epsilon: \" + String(epsilon))\n",
    "            print(test_reward.reward)\n",
    "        }\n",
    "        \n",
    "        // do testing logic... \n",
    "        obs = reset_env(env: env)\n",
    "        games_count+=1\n",
    "        total_reward=0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//reset_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//step_n_times(n: 3, env: env, act: Action(action:1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
